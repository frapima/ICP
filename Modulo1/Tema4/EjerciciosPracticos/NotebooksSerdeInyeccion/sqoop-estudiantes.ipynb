{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de Sqoop\n",
    "\n",
    "En este ejemplo vamos a configurar Sqoop para que realice una serie de importaciones desde una base de datos relacional. \n",
    "\n",
    "Se trata de una base de datos MySQL llamada \"retail_db\" que se encuentra instalada en el contenedor de Cloudera. Esta base de datos tiene las siguientes tablas:\n",
    "\n",
    "- categories\n",
    "- departments\n",
    "- products\n",
    "- order_items\n",
    "- orders\n",
    "- customers\n",
    "\n",
    "<img src=\"files/sqoop-bd.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/sqoop\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p sqoop\n",
    "import os\n",
    "os.chdir(\"sqoop\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer ejemplo: \n",
    "\n",
    "Importamos una tabla, la tabla departments, codificando los datos con Parquet, comprimido con el algoritmo snappy, y creando una tabla en Hive llamada depts. La dejaremos en la base de datos de Hive llamada pruebasqoop, que crearemos ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /user/cloudera/pruebasqoopB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "create database if not exists pruebasqoopB\n",
    "Comment 'BD para pruebas de sqoop'\n",
    "Location '/user/cloudera/pruebasqoopB';\n",
    "\n",
    "describe database extended pruebasqoopB;\n",
    "\n",
    "show databases;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:18:35,412 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 2ms\n",
      "Connecting to jdbc:hive2://localhost:10000/default\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "opBjdbc:hive2://localhost:10000/default> create database if not exists pruebasqo \n",
      "0: jdbc:hive2://localhost:10000/default> Comment 'BD para pruebas de sqoop'\n",
      "0: jdbc:hive2://localhost:10000/default> Location '/user/cloudera/pruebasqoopB'; \n",
      "INFO  : Compiling command(queryId=hive_20190514181818_58299d95-387f-467d-b029-627950a51098): create database if not exists pruebasqoopB\n",
      "Comment 'BD para pruebas de sqoop'\n",
      "Location '/user/cloudera/pruebasqoopB'\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514181818_58299d95-387f-467d-b029-627950a51098); Time taken: 0.538 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514181818_58299d95-387f-467d-b029-627950a51098): create database if not exists pruebasqoopB\n",
      "Comment 'BD para pruebas de sqoop'\n",
      "Location '/user/cloudera/pruebasqoopB'\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514181818_58299d95-387f-467d-b029-627950a51098); Time taken: 0.835 seconds\n",
      "INFO  : OK\n",
      "No rows affected (1.508 seconds)\n",
      "0: jdbc:hive2://localhost:10000/default> \n",
      ";: jdbc:hive2://localhost:10000/default> describe database extended pruebasqoopB \n",
      "INFO  : Compiling command(queryId=hive_20190514181818_0459318e-ef6e-4e4f-ad2a-4b2b8c41a222): describe database extended pruebasqoopB\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:db_name, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer), FieldSchema(name:location, type:string, comment:from deserializer), FieldSchema(name:owner_name, type:string, comment:from deserializer), FieldSchema(name:owner_type, type:string, comment:from deserializer), FieldSchema(name:parameters, type:string, comment:from deserializer)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514181818_0459318e-ef6e-4e4f-ad2a-4b2b8c41a222); Time taken: 0.276 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514181818_0459318e-ef6e-4e4f-ad2a-4b2b8c41a222): describe database extended pruebasqoopB\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514181818_0459318e-ef6e-4e4f-ad2a-4b2b8c41a222); Time taken: 0.053 seconds\n",
      "INFO  : OK\n",
      "+---------------+---------------------------+-------------------------------------------------------------+-------------+-------------+-------------+--+\n",
      "|    db_name    |          comment          |                          location                           | owner_name  | owner_type  | parameters  |\n",
      "+---------------+---------------------------+-------------------------------------------------------------+-------------+-------------+-------------+--+\n",
      "| pruebasqoopb  | BD para pruebas de sqoop  | hdfs://quickstart.cloudera:8020/user/cloudera/pruebasqoopB  | anonymous   | USER        |             |\n",
      "+---------------+---------------------------+-------------------------------------------------------------+-------------+-------------+-------------+--+\n",
      "1 row selected (0.467 seconds)\n",
      "0: jdbc:hive2://localhost:10000/default> \n",
      "0: jdbc:hive2://localhost:10000/default> show databases;\n",
      "INFO  : Compiling command(queryId=hive_20190514181818_d33bdfba-14db-42fe-aec1-6e3d70717fce): show databases\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514181818_d33bdfba-14db-42fe-aec1-6e3d70717fce); Time taken: 0.01 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514181818_d33bdfba-14db-42fe-aec1-6e3d70717fce): show databases\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514181818_d33bdfba-14db-42fe-aec1-6e3d70717fce); Time taken: 0.025 seconds\n",
      "INFO  : OK\n",
      "+-----------------+--+\n",
      "|  database_name  |\n",
      "+-----------------+--+\n",
      "| bdempleados     |\n",
      "| bdlogs          |\n",
      "| bioinformatica  |\n",
      "| clientes        |\n",
      "| default         |\n",
      "| pruebasqoopb    |\n",
      "+-----------------+--+\n",
      "6 rows selected (0.057 seconds)\n",
      "0: jdbc:hive2://localhost:10000/default> \n",
      "Closing: 0: jdbc:hive2://localhost:10000/default\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/default\" -f ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "19/05/14 18:21:11 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0\n",
      "usage: sqoop COMMAND [ARGS]\n",
      "\n",
      "Available commands:\n",
      "  codegen            Generate code to interact with database records\n",
      "  create-hive-table  Import a table definition into Hive\n",
      "  eval               Evaluate a SQL statement and display the results\n",
      "  export             Export an HDFS directory to a database table\n",
      "  help               List available commands\n",
      "  import             Import a table from a database to HDFS\n",
      "  import-all-tables  Import tables from a database to HDFS\n",
      "  import-mainframe   Import datasets from a mainframe server to HDFS\n",
      "  job                Work with saved jobs\n",
      "  list-databases     List available databases on a server\n",
      "  list-tables        List available tables in a database\n",
      "  merge              Merge results of incremental imports\n",
      "  metastore          Run a standalone Sqoop metastore\n",
      "  version            Display version information\n",
      "\n",
      "See 'sqoop help COMMAND' for information on a specific command.\n"
     ]
    }
   ],
   "source": [
    "! sqoop help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "19/05/14 18:21:20 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0\n",
      "usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]\n",
      "\n",
      "Common arguments:\n",
      "   --connect <jdbc-uri>                         Specify JDBC connect\n",
      "                                                string\n",
      "   --connection-manager <class-name>            Specify connection manager\n",
      "                                                class name\n",
      "   --connection-param-file <properties-file>    Specify connection\n",
      "                                                parameters file\n",
      "   --driver <class-name>                        Manually specify JDBC\n",
      "                                                driver class to use\n",
      "   --hadoop-home <hdir>                         Override\n",
      "                                                $HADOOP_MAPRED_HOME_ARG\n",
      "   --hadoop-mapred-home <dir>                   Override\n",
      "                                                $HADOOP_MAPRED_HOME_ARG\n",
      "   --help                                       Print usage instructions\n",
      "-P                                              Read password from console\n",
      "   --password <password>                        Set authentication\n",
      "                                                password\n",
      "   --password-alias <password-alias>            Credential provider\n",
      "                                                password alias\n",
      "   --password-file <password-file>              Set authentication\n",
      "                                                password file path\n",
      "   --relaxed-isolation                          Use read-uncommitted\n",
      "                                                isolation for imports\n",
      "   --skip-dist-cache                            Skip copying jars to\n",
      "                                                distributed cache\n",
      "   --username <username>                        Set authentication\n",
      "                                                username\n",
      "   --verbose                                    Print more information\n",
      "                                                while working\n",
      "\n",
      "Import control arguments:\n",
      "   --append                                                   Imports data\n",
      "                                                              in append\n",
      "                                                              mode\n",
      "   --as-avrodatafile                                          Imports data\n",
      "                                                              to Avro data\n",
      "                                                              files\n",
      "   --as-parquetfile                                           Imports data\n",
      "                                                              to Parquet\n",
      "                                                              files\n",
      "   --as-sequencefile                                          Imports data\n",
      "                                                              to\n",
      "                                                              SequenceFile\n",
      "                                                              s\n",
      "   --as-textfile                                              Imports data\n",
      "                                                              as plain\n",
      "                                                              text\n",
      "                                                              (default)\n",
      "   --autoreset-to-one-mapper                                  Reset the\n",
      "                                                              number of\n",
      "                                                              mappers to\n",
      "                                                              one mapper\n",
      "                                                              if no split\n",
      "                                                              key\n",
      "                                                              available\n",
      "   --boundary-query <statement>                               Set boundary\n",
      "                                                              query for\n",
      "                                                              retrieving\n",
      "                                                              max and min\n",
      "                                                              value of the\n",
      "                                                              primary key\n",
      "   --columns <col,col,col...>                                 Columns to\n",
      "                                                              import from\n",
      "                                                              table\n",
      "   --compression-codec <codec>                                Compression\n",
      "                                                              codec to use\n",
      "                                                              for import\n",
      "   --delete-target-dir                                        Imports data\n",
      "                                                              in delete\n",
      "                                                              mode\n",
      "   --direct                                                   Use direct\n",
      "                                                              import fast\n",
      "                                                              path\n",
      "   --direct-split-size <n>                                    Split the\n",
      "                                                              input stream\n",
      "                                                              every 'n'\n",
      "                                                              bytes when\n",
      "                                                              importing in\n",
      "                                                              direct mode\n",
      "-e,--query <statement>                                        Import\n",
      "                                                              results of\n",
      "                                                              SQL\n",
      "                                                              'statement'\n",
      "   --fetch-size <n>                                           Set number\n",
      "                                                              'n' of rows\n",
      "                                                              to fetch\n",
      "                                                              from the\n",
      "                                                              database\n",
      "                                                              when more\n",
      "                                                              rows are\n",
      "                                                              needed\n",
      "   --inline-lob-limit <n>                                     Set the\n",
      "                                                              maximum size\n",
      "                                                              for an\n",
      "                                                              inline LOB\n",
      "-m,--num-mappers <n>                                          Use 'n' map\n",
      "                                                              tasks to\n",
      "                                                              import in\n",
      "                                                              parallel\n",
      "   --mapreduce-job-name <name>                                Set name for\n",
      "                                                              generated\n",
      "                                                              mapreduce\n",
      "                                                              job\n",
      "   --merge-key <column>                                       Key column\n",
      "                                                              to use to\n",
      "                                                              join results\n",
      "   --split-by <column-name>                                   Column of\n",
      "                                                              the table\n",
      "                                                              used to\n",
      "                                                              split work\n",
      "                                                              units\n",
      "   --split-limit <size>                                       Upper Limit\n",
      "                                                              of rows per\n",
      "                                                              split for\n",
      "                                                              split\n",
      "                                                              columns of\n",
      "                                                              Date/Time/Ti\n",
      "                                                              mestamp and\n",
      "                                                              integer\n",
      "                                                              types. For\n",
      "                                                              date or\n",
      "                                                              timestamp\n",
      "                                                              fields it is\n",
      "                                                              calculated\n",
      "                                                              in seconds.\n",
      "                                                              split-limit\n",
      "                                                              should be\n",
      "                                                              greater than\n",
      "                                                              0\n",
      "   --table <table-name>                                       Table to\n",
      "                                                              read\n",
      "   --target-dir <dir>                                         HDFS plain\n",
      "                                                              table\n",
      "                                                              destination\n",
      "   --validate                                                 Validate the\n",
      "                                                              copy using\n",
      "                                                              the\n",
      "                                                              configured\n",
      "                                                              validator\n",
      "   --validation-failurehandler <validation-failurehandler>    Fully\n",
      "                                                              qualified\n",
      "                                                              class name\n",
      "                                                              for\n",
      "                                                              ValidationFa\n",
      "                                                              ilureHandler\n",
      "   --validation-threshold <validation-threshold>              Fully\n",
      "                                                              qualified\n",
      "                                                              class name\n",
      "                                                              for\n",
      "                                                              ValidationTh\n",
      "                                                              reshold\n",
      "   --validator <validator>                                    Fully\n",
      "                                                              qualified\n",
      "                                                              class name\n",
      "                                                              for the\n",
      "                                                              Validator\n",
      "   --warehouse-dir <dir>                                      HDFS parent\n",
      "                                                              for table\n",
      "                                                              destination\n",
      "   --where <where clause>                                     WHERE clause\n",
      "                                                              to use\n",
      "                                                              during\n",
      "                                                              import\n",
      "-z,--compress                                                 Enable\n",
      "                                                              compression\n",
      "\n",
      "Incremental import arguments:\n",
      "   --check-column <column>        Source column to check for incremental\n",
      "                                  change\n",
      "   --incremental <import-type>    Define an incremental import of type\n",
      "                                  'append' or 'lastmodified'\n",
      "   --last-value <value>           Last imported value in the incremental\n",
      "                                  check column\n",
      "\n",
      "Output line formatting arguments:\n",
      "   --enclosed-by <char>               Sets a required field enclosing\n",
      "                                      character\n",
      "   --escaped-by <char>                Sets the escape character\n",
      "   --fields-terminated-by <char>      Sets the field separator character\n",
      "   --lines-terminated-by <char>       Sets the end-of-line character\n",
      "   --mysql-delimiters                 Uses MySQL's default delimiter set:\n",
      "                                      fields: ,  lines: \\n  escaped-by: \\\n",
      "                                      optionally-enclosed-by: '\n",
      "   --optionally-enclosed-by <char>    Sets a field enclosing character\n",
      "\n",
      "Input parsing arguments:\n",
      "   --input-enclosed-by <char>               Sets a required field encloser\n",
      "   --input-escaped-by <char>                Sets the input escape\n",
      "                                            character\n",
      "   --input-fields-terminated-by <char>      Sets the input field separator\n",
      "   --input-lines-terminated-by <char>       Sets the input end-of-line\n",
      "                                            char\n",
      "   --input-optionally-enclosed-by <char>    Sets a field enclosing\n",
      "                                            character\n",
      "\n",
      "Hive arguments:\n",
      "   --create-hive-table                         Fail if the target hive\n",
      "                                               table exists\n",
      "   --hive-database <database-name>             Sets the database name to\n",
      "                                               use when importing to hive\n",
      "   --hive-delims-replacement <arg>             Replace Hive record \\0x01\n",
      "                                               and row delimiters (\\n\\r)\n",
      "                                               from imported string fields\n",
      "                                               with user-defined string\n",
      "   --hive-drop-import-delims                   Drop Hive record \\0x01 and\n",
      "                                               row delimiters (\\n\\r) from\n",
      "                                               imported string fields\n",
      "   --hive-home <dir>                           Override $HIVE_HOME\n",
      "   --hive-import                               Import tables into Hive\n",
      "                                               (Uses Hive's default\n",
      "                                               delimiters if none are\n",
      "                                               set.)\n",
      "   --hive-overwrite                            Overwrite existing data in\n",
      "                                               the Hive table\n",
      "   --hive-partition-key <partition-key>        Sets the partition key to\n",
      "                                               use when importing to hive\n",
      "   --hive-partition-value <partition-value>    Sets the partition value to\n",
      "                                               use when importing to hive\n",
      "   --hive-table <table-name>                   Sets the table name to use\n",
      "                                               when importing to hive\n",
      "   --map-column-hive <arg>                     Override mapping for\n",
      "                                               specific column to hive\n",
      "                                               types.\n",
      "\n",
      "HBase arguments:\n",
      "   --column-family <family>    Sets the target column family for the\n",
      "                               import\n",
      "   --hbase-bulkload            Enables HBase bulk loading\n",
      "   --hbase-create-table        If specified, create missing HBase tables\n",
      "   --hbase-row-key <col>       Specifies which input column to use as the\n",
      "                               row key\n",
      "   --hbase-table <table>       Import to <table> in HBase\n",
      "\n",
      "HCatalog arguments:\n",
      "   --hcatalog-database <arg>                        HCatalog database name\n",
      "   --hcatalog-home <hdir>                           Override $HCAT_HOME\n",
      "   --hcatalog-partition-keys <partition-key>        Sets the partition\n",
      "                                                    keys to use when\n",
      "                                                    importing to hive\n",
      "   --hcatalog-partition-values <partition-value>    Sets the partition\n",
      "                                                    values to use when\n",
      "                                                    importing to hive\n",
      "   --hcatalog-table <arg>                           HCatalog table name\n",
      "   --hive-home <dir>                                Override $HIVE_HOME\n",
      "   --hive-partition-key <partition-key>             Sets the partition key\n",
      "                                                    to use when importing\n",
      "                                                    to hive\n",
      "   --hive-partition-value <partition-value>         Sets the partition\n",
      "                                                    value to use when\n",
      "                                                    importing to hive\n",
      "   --map-column-hive <arg>                          Override mapping for\n",
      "                                                    specific column to\n",
      "                                                    hive types.\n",
      "\n",
      "HCatalog import specific options:\n",
      "   --create-hcatalog-table            Create HCatalog before import\n",
      "   --hcatalog-storage-stanza <arg>    HCatalog storage stanza for table\n",
      "                                      creation\n",
      "\n",
      "Accumulo arguments:\n",
      "   --accumulo-batch-size <size>          Batch size in bytes\n",
      "   --accumulo-column-family <family>     Sets the target column family for\n",
      "                                         the import\n",
      "   --accumulo-create-table               If specified, create missing\n",
      "                                         Accumulo tables\n",
      "   --accumulo-instance <instance>        Accumulo instance name.\n",
      "   --accumulo-max-latency <latency>      Max write latency in milliseconds\n",
      "   --accumulo-password <password>        Accumulo password.\n",
      "   --accumulo-row-key <col>              Specifies which input column to\n",
      "                                         use as the row key\n",
      "   --accumulo-table <table>              Import to <table> in Accumulo\n",
      "   --accumulo-user <user>                Accumulo user name.\n",
      "   --accumulo-visibility <vis>           Visibility token to be applied to\n",
      "                                         all rows imported\n",
      "   --accumulo-zookeepers <zookeepers>    Comma-separated list of\n",
      "                                         zookeepers (host:port)\n",
      "\n",
      "Code generation arguments:\n",
      "   --bindir <dir>                        Output directory for compiled\n",
      "                                         objects\n",
      "   --class-name <name>                   Sets the generated class name.\n",
      "                                         This overrides --package-name.\n",
      "                                         When combined with --jar-file,\n",
      "                                         sets the input class.\n",
      "   --input-null-non-string <null-str>    Input null non-string\n",
      "                                         representation\n",
      "   --input-null-string <null-str>        Input null string representation\n",
      "   --jar-file <file>                     Disable code generation; use\n",
      "                                         specified jar\n",
      "   --map-column-java <arg>               Override mapping for specific\n",
      "                                         columns to java types\n",
      "   --null-non-string <null-str>          Null non-string representation\n",
      "   --null-string <null-str>              Null string representation\n",
      "   --outdir <dir>                        Output directory for generated\n",
      "                                         code\n",
      "   --package-name <name>                 Put auto-generated classes in\n",
      "                                         this package\n",
      "\n",
      "Generic Hadoop command-line arguments:\n",
      "(must preceed any tool-specific arguments)\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n",
      "\n",
      "At minimum, you must specify --connect and --table\n",
      "Arguments to mysqldump and other subprograms may be supplied\n",
      "after a '--' on the command line.\n"
     ]
    }
   ],
   "source": [
    "! sqoop help import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "sqoop import \\\n",
    "    --connect jdbc:mysql://quickstart:3306/retail_db \\\n",
    "    --username=retail_dba \\\n",
    "    --password=cloudera \\\n",
    "    --table departments \\\n",
    "    --compression-codec=snappy \\\n",
    "    --as-parquetfile \\\n",
    "    --hive-import \\\n",
    "    --hive-database pruebasqoopB \\\n",
    "    --hive-table depts \\\n",
    "    --hive-overwrite;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "19/05/14 18:21:41 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0\n",
      "19/05/14 18:21:41 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/05/14 18:21:41 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override\n",
      "19/05/14 18:21:41 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.\n",
      "19/05/14 18:21:42 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "19/05/14 18:21:42 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:21:42 INFO tool.CodeGenTool: Will generate java class as codegen_departments\n",
      "19/05/14 18:21:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:21:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:21:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/2614717abad66a70bcc47b540f00a63a/codegen_departments.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:21:44 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/2614717abad66a70bcc47b540f00a63a/codegen_departments.jar\n",
      "19/05/14 18:21:44 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "19/05/14 18:21:44 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "19/05/14 18:21:44 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "19/05/14 18:21:44 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "19/05/14 18:21:44 INFO mapreduce.ImportJobBase: Beginning import of departments\n",
      "19/05/14 18:21:44 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:21:44 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "19/05/14 18:21:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:21:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:21:46 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:21:46 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:21:47 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:21:47 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:21:47 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:21:47 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:21:47 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:21:47 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:21:47 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:21:47 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:21:47 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:21:47 INFO hive.HiveManagedMetadataProvider: Creating a managed Hive table named: depts\n",
      "19/05/14 18:21:47 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:21:47 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:21:47 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:21:47 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:21:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/05/14 18:21:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:22:00 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:22:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`department_id`), MAX(`department_id`) FROM `departments`\n",
      "19/05/14 18:22:00 INFO db.IntegerSplitter: Split size: 1; Num splits: 4 from: 2 to: 7\n",
      "19/05/14 18:22:00 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:22:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0001\n",
      "19/05/14 18:22:01 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0001\n",
      "19/05/14 18:22:01 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0001/\n",
      "19/05/14 18:22:01 INFO mapreduce.Job: Running job: job_1557856596151_0001\n",
      "19/05/14 18:22:16 INFO mapreduce.Job: Job job_1557856596151_0001 running in uber mode : false\n",
      "19/05/14 18:22:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:22:30 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/14 18:22:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:22:33 INFO mapreduce.Job: Job job_1557856596151_0001 completed successfully\n",
      "19/05/14 18:22:33 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=837480\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=18005\n",
      "\t\tHDFS: Number of bytes written=7664\n",
      "\t\tHDFS: Number of read operations=192\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=40\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44451\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=44451\n",
      "\t\tTotal vcore-seconds taken by all map tasks=44451\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=45517824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tInput split bytes=481\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=1181\n",
      "\t\tCPU time spent (ms)=12350\n",
      "\t\tPhysical memory (bytes) snapshot=1719291904\n",
      "\t\tVirtual memory (bytes) snapshot=5606109184\n",
      "\t\tTotal committed heap usage (bytes)=1421344768\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "19/05/14 18:22:33 INFO mapreduce.ImportJobBase: Transferred 7.4844 KB in 44.0185 seconds (174.1086 bytes/sec)\n",
      "19/05/14 18:22:33 INFO mapreduce.ImportJobBase: Retrieved 6 records.\n"
     ]
    }
   ],
   "source": [
    "! sh ./ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:23:33,055 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 2ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoopB\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "INFO  : Compiling command(queryId=hive_20190514182323_730ddd56-b110-487b-9122-493c5024d304): show tables\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514182323_730ddd56-b110-487b-9122-493c5024d304); Time taken: 0.017 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514182323_730ddd56-b110-487b-9122-493c5024d304): show tables\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514182323_730ddd56-b110-487b-9122-493c5024d304); Time taken: 0.066 seconds\n",
      "INFO  : OK\n",
      "+-----------+--+\n",
      "| tab_name  |\n",
      "+-----------+--+\n",
      "| depts     |\n",
      "+-----------+--+\n",
      "1 row selected (0.184 seconds)\n",
      "Beeline version 1.1.0-cdh5.7.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoopB\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoopB\" -e \"show tables;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:23:42,833 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoopB\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "INFO  : Compiling command(queryId=hive_20190514182323_ff09f2cd-519e-42d0-96ae-6cebd114a883): select * from depts\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:depts.department_id, type:int, comment:null), FieldSchema(name:depts.department_name, type:string, comment:null)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514182323_ff09f2cd-519e-42d0-96ae-6cebd114a883); Time taken: 0.392 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514182323_ff09f2cd-519e-42d0-96ae-6cebd114a883): select * from depts\n",
      "INFO  : Completed executing command(queryId=hive_20190514182323_ff09f2cd-519e-42d0-96ae-6cebd114a883); Time taken: 0.0 seconds\n",
      "INFO  : OK\n",
      "+----------------------+------------------------+--+\n",
      "| depts.department_id  | depts.department_name  |\n",
      "+----------------------+------------------------+--+\n",
      "| 6                    | Outdoors               |\n",
      "| 7                    | Fan Shop               |\n",
      "| 2                    | Fitness                |\n",
      "| 3                    | Footwear               |\n",
      "| 4                    | Apparel                |\n",
      "| 5                    | Golf                   |\n",
      "+----------------------+------------------------+--+\n",
      "6 rows selected (0.907 seconds)\n",
      "Beeline version 1.1.0-cdh5.7.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoopB\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoopB\" -e \"select * from depts;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/cloudera/pruebasqoopB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "drwxr-xr-x   - root cloudera            0 2019-05-14 18:21 /user/cloudera/pruebasqoopB/depts/.metadata\r\n",
      "drwxr-xr-x   - root cloudera            0 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts/.signals\r\n",
      "-rw-r--r--   1 root supergroup        745 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts/957b7c9e-c728-46c1-b6e4-8764ecea610d.parquet\r\n",
      "-rw-r--r--   1 root supergroup        742 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts/9b20ed1e-19bb-4b59-b9c7-7582407aad95.parquet\r\n",
      "-rw-r--r--   1 root supergroup        722 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts/9c3fa633-c337-4a6e-8f2f-4161e4797075.parquet\r\n",
      "-rw-r--r--   1 root supergroup        707 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts/e27ef0e9-7c20-4b55-96e0-749c8b41dd3c.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/cloudera/pruebasqoopB/depts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo ejemplo: \n",
    "\n",
    "Importamos solamente aquellas de las filas de la tabla departments que tengan como department_name el valor Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "sqoop import \\\n",
    "    --connect jdbc:mysql://quickstart:3306/retail_db \\\n",
    "    --username=retail_dba \\\n",
    "    --password=cloudera \\\n",
    "    --table departments \\\n",
    "    --compression-codec=snappy \\\n",
    "    --as-parquetfile \\\n",
    "    --hive-import \\\n",
    "    --hive-database pruebasqoopB \\\n",
    "    --hive-table deptFitness \\\n",
    "    --where \"department_name = 'Fitness'\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "19/05/14 18:26:38 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0\n",
      "19/05/14 18:26:38 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/05/14 18:26:38 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override\n",
      "19/05/14 18:26:38 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.\n",
      "19/05/14 18:26:39 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "19/05/14 18:26:39 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:26:39 INFO tool.CodeGenTool: Will generate java class as codegen_departments\n",
      "19/05/14 18:26:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:26:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:26:39 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/ad867c3bb418006c34c9af455a34d51e/codegen_departments.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:26:40 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ad867c3bb418006c34c9af455a34d51e/codegen_departments.jar\n",
      "19/05/14 18:26:40 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "19/05/14 18:26:40 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "19/05/14 18:26:40 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "19/05/14 18:26:40 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "19/05/14 18:26:40 INFO mapreduce.ImportJobBase: Beginning import of departments\n",
      "19/05/14 18:26:40 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:26:40 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "19/05/14 18:26:41 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:26:41 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:26:42 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:26:42 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:26:42 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:26:42 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:26:42 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:26:42 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:26:42 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:26:42 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:26:42 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:26:42 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:26:42 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:26:42 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:26:42 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:26:42 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:26:42 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:26:42 INFO hive.HiveManagedMetadataProvider: Creating a managed Hive table named: deptFitness\n",
      "19/05/14 18:26:42 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "19/05/14 18:26:42 INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083\n",
      "19/05/14 18:26:42 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/05/14 18:26:42 INFO hive.metastore: Connected to metastore.\n",
      "19/05/14 18:26:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/05/14 18:26:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:26:48 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:26:48 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`department_id`), MAX(`department_id`) FROM `departments` WHERE ( department_name = 'Fitness' )\n",
      "19/05/14 18:26:48 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 2 to: 2\n",
      "19/05/14 18:26:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/05/14 18:26:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0003\n",
      "19/05/14 18:26:50 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0003\n",
      "19/05/14 18:26:50 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0003/\n",
      "19/05/14 18:26:50 INFO mapreduce.Job: Running job: job_1557856596151_0003\n",
      "19/05/14 18:27:01 INFO mapreduce.Job: Job job_1557856596151_0003 running in uber mode : false\n",
      "19/05/14 18:27:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:27:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:27:09 INFO mapreduce.Job: Job job_1557856596151_0003 completed successfully\n",
      "19/05/14 18:27:09 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=209712\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4502\n",
      "\t\tHDFS: Number of bytes written=1909\n",
      "\t\tHDFS: Number of read operations=48\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=10\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5071\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=5071\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5071\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5192704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tInput split bytes=121\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=78\n",
      "\t\tCPU time spent (ms)=2660\n",
      "\t\tPhysical memory (bytes) snapshot=427163648\n",
      "\t\tVirtual memory (bytes) snapshot=1398530048\n",
      "\t\tTotal committed heap usage (bytes)=358088704\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "19/05/14 18:27:09 INFO mapreduce.ImportJobBase: Transferred 1.8643 KB in 25.7295 seconds (74.1949 bytes/sec)\n",
      "19/05/14 18:27:09 INFO mapreduce.ImportJobBase: Retrieved 1 records.\n"
     ]
    }
   ],
   "source": [
    "! sh ./ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:27:44,909 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 2ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoopB\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "INFO  : Compiling command(queryId=hive_20190514182727_341ec898-c9b7-4b9c-8178-3f0db34e54a3): show tables\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514182727_341ec898-c9b7-4b9c-8178-3f0db34e54a3); Time taken: 0.007 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514182727_341ec898-c9b7-4b9c-8178-3f0db34e54a3): show tables\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514182727_341ec898-c9b7-4b9c-8178-3f0db34e54a3); Time taken: 0.026 seconds\n",
      "INFO  : OK\n",
      "+--------------+--+\n",
      "|   tab_name   |\n",
      "+--------------+--+\n",
      "| deptfitness  |\n",
      "| depts        |\n",
      "+--------------+--+\n",
      "2 rows selected (0.144 seconds)\n",
      "Beeline version 1.1.0-cdh5.7.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoopB\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoopB\" -e \"show tables;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:27 /user/cloudera/pruebasqoopB/deptfitness\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:22 /user/cloudera/pruebasqoopB/depts\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/cloudera/pruebasqoopB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:28:13,266 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoopB\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "INFO  : Compiling command(queryId=hive_20190514182828_ad9691cb-23c6-4745-afd6-0ed405011333): select * from  deptfitness\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:deptfitness.department_id, type:int, comment:null), FieldSchema(name:deptfitness.department_name, type:string, comment:null)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514182828_ad9691cb-23c6-4745-afd6-0ed405011333); Time taken: 0.075 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514182828_ad9691cb-23c6-4745-afd6-0ed405011333): select * from  deptfitness\n",
      "INFO  : Completed executing command(queryId=hive_20190514182828_ad9691cb-23c6-4745-afd6-0ed405011333); Time taken: 0.0 seconds\n",
      "INFO  : OK\n",
      "+----------------------------+------------------------------+--+\n",
      "| deptfitness.department_id  | deptfitness.department_name  |\n",
      "+----------------------------+------------------------------+--+\n",
      "| 2                          | Fitness                      |\n",
      "+----------------------------+------------------------------+--+\n",
      "1 row selected (0.218 seconds)\n",
      "Beeline version 1.1.0-cdh5.7.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoopB\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoopB\" -e \"select * from  deptfitness;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:28:43,778 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 2ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoopB\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "INFO  : Compiling command(queryId=hive_20190514182828_200a6059-8971-4fe3-88df-850546fa6058): describe  deptfitness\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col_name, type:string, comment:from deserializer), FieldSchema(name:data_type, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514182828_200a6059-8971-4fe3-88df-850546fa6058); Time taken: 0.074 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514182828_200a6059-8971-4fe3-88df-850546fa6058): describe  deptfitness\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514182828_200a6059-8971-4fe3-88df-850546fa6058); Time taken: 0.02 seconds\n",
      "INFO  : OK\n",
      "+------------------+------------+----------+--+\n",
      "|     col_name     | data_type  | comment  |\n",
      "+------------------+------------+----------+--+\n",
      "| department_id    | int        |          |\n",
      "| department_name  | string     |          |\n",
      "+------------------+------------+----------+--+\n",
      "2 rows selected (0.198 seconds)\n",
      "Beeline version 1.1.0-cdh5.7.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoopB\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoopB\" -e \"describe  deptfitness;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio propuesto\n",
    "\n",
    "1. Realiza la importación de la base de datos retail_db entera, incluyendo todas las tablas. El destino de esta base de datos debe ser la carpeta /user/cloudera/retailentera (que hay que crear antes de nada). Los datos deben estar codificados en avro, sin compresión. \n",
    "\n",
    "2. Crea una tabla externa Hive al menos para una de las tablas de la base de datos. \n",
    "\n",
    "3. Crea una segunda tabla externa en Hive, en la que insertaremos el resultado de una consulta select que realizaremos sobre la tabla externa Hive creada en el punto anterior. La consulta select puede ser cualquiera (selecciona los items cuyo id sea 1, por ejemplo).\n",
    "\n",
    "4. Crea una nueva tabla en la base de datos relacional.\n",
    "\n",
    "5. Exporta la tabla Hive creada en el punto 3 en la tabla relacional creada en el punto 4. \n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "Para el punto 4, puedes utilizar estas órdenes desde un terminal (es posible utilizar una tabla distinta):\n",
    "\n",
    "Conéctate a tu contenedor con esta orden:\n",
    "\n",
    "docker exec -ti IDCONTENEDOR  /bin/bash\n",
    "\n",
    "Y luego emite estas órdenes (elimina lo que haytras las -->):\n",
    "\n",
    "mysql -u retail_dba -p         \n",
    "--> nos conectamos a mysql, utilizando el usuario retail_dba. Tras ejecutar esta orden deberemos introducir la clave, que es cloudera\n",
    "\n",
    "use retail_db;\n",
    "--> indicamos que deseamos utilizar la base de datos retail_db\n",
    "\n",
    "\n",
    "CREATE TABLE depsB (\n",
    "    department_id int,\n",
    "    department_name varchar(45),\n",
    "    PRIMARY KEY (department_id)\n",
    ");\n",
    "--> Creamos una tabla, que será donde dejaremos los datos exportados desde HDFS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /user/cloudera/retailentera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "# TU CODIGO PARA EL PUNTO 1 VA AQUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "19/05/14 18:44:56 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0\n",
      "19/05/14 18:44:56 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/05/14 18:44:57 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "19/05/14 18:44:57 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:44:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `categories` AS t LIMIT 1\n",
      "19/05/14 18:44:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `categories` AS t LIMIT 1\n",
      "19/05/14 18:44:57 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/categories.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:44:58 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/categories.jar\n",
      "19/05/14 18:44:58 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "19/05/14 18:44:58 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "19/05/14 18:44:58 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "19/05/14 18:44:58 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "19/05/14 18:44:58 INFO mapreduce.ImportJobBase: Beginning import of categories\n",
      "19/05/14 18:44:58 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:44:59 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "19/05/14 18:44:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `categories` AS t LIMIT 1\n",
      "19/05/14 18:44:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `categories` AS t LIMIT 1\n",
      "19/05/14 18:44:59 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/categories.avsc\n",
      "19/05/14 18:44:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/05/14 18:44:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:45:01 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:45:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`category_id`), MAX(`category_id`) FROM `categories`\n",
      "19/05/14 18:45:01 INFO db.IntegerSplitter: Split size: 14; Num splits: 4 from: 1 to: 58\n",
      "19/05/14 18:45:01 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:45:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0004\n",
      "19/05/14 18:45:01 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0004\n",
      "19/05/14 18:45:01 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0004/\n",
      "19/05/14 18:45:01 INFO mapreduce.Job: Running job: job_1557856596151_0004\n",
      "19/05/14 18:45:08 INFO mapreduce.Job: Job job_1557856596151_0004 running in uber mode : false\n",
      "19/05/14 18:45:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:45:15 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/05/14 18:45:16 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/05/14 18:45:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:45:17 INFO mapreduce.Job: Job job_1557856596151_0004 completed successfully\n",
      "19/05/14 18:45:17 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=553948\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=472\n",
      "\t\tHDFS: Number of bytes written=3022\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19741\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=19741\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19741\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=20214784\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=58\n",
      "\t\tMap output records=58\n",
      "\t\tInput split bytes=472\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=222\n",
      "\t\tCPU time spent (ms)=4080\n",
      "\t\tPhysical memory (bytes) snapshot=794447872\n",
      "\t\tVirtual memory (bytes) snapshot=5487509504\n",
      "\t\tTotal committed heap usage (bytes)=832569344\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3022\n",
      "19/05/14 18:45:17 INFO mapreduce.ImportJobBase: Transferred 2.9512 KB in 17.472 seconds (172.9626 bytes/sec)\n",
      "19/05/14 18:45:17 INFO mapreduce.ImportJobBase: Retrieved 58 records.\n",
      "19/05/14 18:45:17 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:45:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `customers` AS t LIMIT 1\n",
      "19/05/14 18:45:17 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/customers.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:45:17 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/customers.jar\n",
      "19/05/14 18:45:17 INFO mapreduce.ImportJobBase: Beginning import of customers\n",
      "19/05/14 18:45:17 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:45:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `customers` AS t LIMIT 1\n",
      "19/05/14 18:45:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `customers` AS t LIMIT 1\n",
      "19/05/14 18:45:17 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/customers.avsc\n",
      "19/05/14 18:45:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:45:19 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:45:19 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`customer_id`), MAX(`customer_id`) FROM `customers`\n",
      "19/05/14 18:45:19 INFO db.IntegerSplitter: Split size: 3108; Num splits: 4 from: 1 to: 12435\n",
      "19/05/14 18:45:19 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:45:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0005\n",
      "19/05/14 18:45:19 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0005\n",
      "19/05/14 18:45:19 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0005/\n",
      "19/05/14 18:45:19 INFO mapreduce.Job: Running job: job_1557856596151_0005\n",
      "19/05/14 18:45:25 INFO mapreduce.Job: Job job_1557856596151_0005 running in uber mode : false\n",
      "19/05/14 18:45:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:45:32 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/05/14 18:45:33 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/14 18:45:34 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/05/14 18:45:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:45:36 INFO mapreduce.Job: Job job_1557856596151_0005 completed successfully\n",
      "19/05/14 18:45:36 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=557000\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=487\n",
      "\t\tHDFS: Number of bytes written=1035959\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21458\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=21458\n",
      "\t\tTotal vcore-seconds taken by all map tasks=21458\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=21972992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12435\n",
      "\t\tMap output records=12435\n",
      "\t\tInput split bytes=487\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=226\n",
      "\t\tCPU time spent (ms)=9330\n",
      "\t\tPhysical memory (bytes) snapshot=944713728\n",
      "\t\tVirtual memory (bytes) snapshot=5506379776\n",
      "\t\tTotal committed heap usage (bytes)=904921088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1035959\n",
      "19/05/14 18:45:36 INFO mapreduce.ImportJobBase: Transferred 1,011.6787 KB in 18.7116 seconds (54.0669 KB/sec)\n",
      "19/05/14 18:45:36 INFO mapreduce.ImportJobBase: Retrieved 12435 records.\n",
      "19/05/14 18:45:36 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:45:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:45:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/departments.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:45:36 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/departments.jar\n",
      "19/05/14 18:45:36 INFO mapreduce.ImportJobBase: Beginning import of departments\n",
      "19/05/14 18:45:36 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:45:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:45:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `departments` AS t LIMIT 1\n",
      "19/05/14 18:45:36 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/departments.avsc\n",
      "19/05/14 18:45:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:45:38 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:45:38 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`department_id`), MAX(`department_id`) FROM `departments`\n",
      "19/05/14 18:45:38 INFO db.IntegerSplitter: Split size: 1; Num splits: 4 from: 2 to: 7\n",
      "19/05/14 18:45:38 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:45:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0006\n",
      "19/05/14 18:45:38 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0006\n",
      "19/05/14 18:45:38 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0006/\n",
      "19/05/14 18:45:38 INFO mapreduce.Job: Running job: job_1557856596151_0006\n",
      "19/05/14 18:45:44 INFO mapreduce.Job: Job job_1557856596151_0006 running in uber mode : false\n",
      "19/05/14 18:45:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:45:51 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/05/14 18:45:52 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/14 18:45:53 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/05/14 18:45:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:45:54 INFO mapreduce.Job: Job job_1557856596151_0006 completed successfully\n",
      "19/05/14 18:45:54 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=553440\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=481\n",
      "\t\tHDFS: Number of bytes written=1598\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17690\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=17690\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17690\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18114560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tInput split bytes=481\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=214\n",
      "\t\tCPU time spent (ms)=4500\n",
      "\t\tPhysical memory (bytes) snapshot=806117376\n",
      "\t\tVirtual memory (bytes) snapshot=5485441024\n",
      "\t\tTotal committed heap usage (bytes)=898105344\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1598\n",
      "19/05/14 18:45:54 INFO mapreduce.ImportJobBase: Transferred 1.5605 KB in 17.2702 seconds (92.5296 bytes/sec)\n",
      "19/05/14 18:45:54 INFO mapreduce.ImportJobBase: Retrieved 6 records.\n",
      "19/05/14 18:45:54 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:45:54 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `order_items` AS t LIMIT 1\n",
      "19/05/14 18:45:54 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/order_items.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:45:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/order_items.jar\n",
      "19/05/14 18:45:54 INFO mapreduce.ImportJobBase: Beginning import of order_items\n",
      "19/05/14 18:45:54 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:45:54 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `order_items` AS t LIMIT 1\n",
      "19/05/14 18:45:54 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `order_items` AS t LIMIT 1\n",
      "19/05/14 18:45:54 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/order_items.avsc\n",
      "19/05/14 18:45:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:45:56 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:45:56 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`order_item_id`), MAX(`order_item_id`) FROM `order_items`\n",
      "19/05/14 18:45:56 INFO db.IntegerSplitter: Split size: 43049; Num splits: 4 from: 1 to: 172198\n",
      "19/05/14 18:45:56 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:45:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0007\n",
      "19/05/14 18:45:56 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0007\n",
      "19/05/14 18:45:56 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0007/\n",
      "19/05/14 18:45:56 INFO mapreduce.Job: Running job: job_1557856596151_0007\n",
      "19/05/14 18:46:02 INFO mapreduce.Job: Job job_1557856596151_0007 running in uber mode : false\n",
      "19/05/14 18:46:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:46:10 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/05/14 18:46:11 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/14 18:46:12 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/05/14 18:46:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:46:13 INFO mapreduce.Job: Job job_1557856596151_0007 completed successfully\n",
      "19/05/14 18:46:13 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=555816\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=512\n",
      "\t\tHDFS: Number of bytes written=3935616\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24998\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=24998\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24998\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25597952\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=172198\n",
      "\t\tMap output records=172198\n",
      "\t\tInput split bytes=512\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=395\n",
      "\t\tCPU time spent (ms)=12940\n",
      "\t\tPhysical memory (bytes) snapshot=1555664896\n",
      "\t\tVirtual memory (bytes) snapshot=5477879808\n",
      "\t\tTotal committed heap usage (bytes)=1437597696\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3935616\n",
      "19/05/14 18:46:13 INFO mapreduce.ImportJobBase: Transferred 3.7533 MB in 19.0134 seconds (202.1406 KB/sec)\n",
      "19/05/14 18:46:13 INFO mapreduce.ImportJobBase: Retrieved 172198 records.\n",
      "19/05/14 18:46:13 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:46:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `orders` AS t LIMIT 1\n",
      "19/05/14 18:46:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/orders.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:46:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/orders.jar\n",
      "19/05/14 18:46:14 INFO mapreduce.ImportJobBase: Beginning import of orders\n",
      "19/05/14 18:46:14 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:46:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `orders` AS t LIMIT 1\n",
      "19/05/14 18:46:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `orders` AS t LIMIT 1\n",
      "19/05/14 18:46:14 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/orders.avsc\n",
      "19/05/14 18:46:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:46:15 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:46:15 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`order_id`), MAX(`order_id`) FROM `orders`\n",
      "19/05/14 18:46:15 INFO db.IntegerSplitter: Split size: 17220; Num splits: 4 from: 1 to: 68883\n",
      "19/05/14 18:46:15 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:46:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0008\n",
      "19/05/14 18:46:16 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0008\n",
      "19/05/14 18:46:16 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0008/\n",
      "19/05/14 18:46:16 INFO mapreduce.Job: Running job: job_1557856596151_0008\n",
      "19/05/14 18:46:22 INFO mapreduce.Job: Job job_1557856596151_0008 running in uber mode : false\n",
      "19/05/14 18:46:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:46:30 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/05/14 18:46:32 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/14 18:46:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:46:34 INFO mapreduce.Job: Job job_1557856596151_0008 completed successfully\n",
      "19/05/14 18:46:34 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=554172\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=469\n",
      "\t\tHDFS: Number of bytes written=1781437\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28441\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=28441\n",
      "\t\tTotal vcore-seconds taken by all map tasks=28441\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=29123584\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=68883\n",
      "\t\tMap output records=68883\n",
      "\t\tInput split bytes=469\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=379\n",
      "\t\tCPU time spent (ms)=13800\n",
      "\t\tPhysical memory (bytes) snapshot=1054994432\n",
      "\t\tVirtual memory (bytes) snapshot=5482057728\n",
      "\t\tTotal committed heap usage (bytes)=902823936\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1781437\n",
      "19/05/14 18:46:34 INFO mapreduce.ImportJobBase: Transferred 1.6989 MB in 20.0628 seconds (86.712 KB/sec)\n",
      "19/05/14 18:46:34 INFO mapreduce.ImportJobBase: Retrieved 68883 records.\n",
      "19/05/14 18:46:34 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:46:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `products` AS t LIMIT 1\n",
      "19/05/14 18:46:34 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/products.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:46:34 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/products.jar\n",
      "19/05/14 18:46:34 INFO mapreduce.ImportJobBase: Beginning import of products\n",
      "19/05/14 18:46:34 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:46:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `products` AS t LIMIT 1\n",
      "19/05/14 18:46:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `products` AS t LIMIT 1\n",
      "19/05/14 18:46:34 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-root/compile/3c9b40cd774583d996dadcb380660399/products.avsc\n",
      "19/05/14 18:46:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:46:35 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/05/14 18:46:35 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`product_id`), MAX(`product_id`) FROM `products`\n",
      "19/05/14 18:46:35 INFO db.IntegerSplitter: Split size: 336; Num splits: 4 from: 1 to: 1345\n",
      "19/05/14 18:46:36 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/05/14 18:46:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0009\n",
      "19/05/14 18:46:36 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0009\n",
      "19/05/14 18:46:36 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0009/\n",
      "19/05/14 18:46:36 INFO mapreduce.Job: Running job: job_1557856596151_0009\n",
      "19/05/14 18:46:42 INFO mapreduce.Job: Job job_1557856596151_0009 running in uber mode : false\n",
      "19/05/14 18:46:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:46:48 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/05/14 18:46:49 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/14 18:46:50 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/05/14 18:46:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:46:53 INFO mapreduce.Job: Job job_1557856596151_0009 completed successfully\n",
      "19/05/14 18:46:53 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=555408\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=474\n",
      "\t\tHDFS: Number of bytes written=178092\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17981\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=17981\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17981\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18412544\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1345\n",
      "\t\tMap output records=1345\n",
      "\t\tInput split bytes=474\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=161\n",
      "\t\tCPU time spent (ms)=4890\n",
      "\t\tPhysical memory (bytes) snapshot=812433408\n",
      "\t\tVirtual memory (bytes) snapshot=5469016064\n",
      "\t\tTotal committed heap usage (bytes)=897581056\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=178092\n",
      "19/05/14 18:46:53 INFO mapreduce.ImportJobBase: Transferred 173.918 KB in 19.0064 seconds (9.1505 KB/sec)\n",
      "19/05/14 18:46:53 INFO mapreduce.ImportJobBase: Retrieved 1345 records.\n"
     ]
    }
   ],
   "source": [
    "! sh ./ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:45 /user/cloudera/retailentera/categories\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:45 /user/cloudera/retailentera/customers\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:45 /user/cloudera/retailentera/departments\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:46 /user/cloudera/retailentera/order_items\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:46 /user/cloudera/retailentera/orders\r\n",
      "drwxr-xr-x   - root cloudera          0 2019-05-14 18:46 /user/cloudera/retailentera/products\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/cloudera/retailentera/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "create external table departmentssqoopavro (\n",
    "  department_id  int,   \n",
    "  department_name  STRING) \n",
    "STORED AS AVRO\n",
    "LOCATION '/user/cloudera/retailentera/departments';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:48:19,136 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 2ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoop\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "oopavro (ive2://localhost:10000/pruebasqoop> create external table departmentssq \n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop>   department_id  int,   \n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop>   department_name  STRING) \n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop> STORED AS AVRO\n",
      "ra/departments';localhost:10000/pruebasqoop> LOCATION '/user/cloudera/retailente \n",
      "INFO  : Compiling command(queryId=hive_20190514184848_bb5cf35f-86e6-4eca-8c57-1697c47b8bce): create external table departmentssqoopavro (\n",
      "  department_id  int,   \n",
      "  department_name  STRING) \n",
      "STORED AS AVRO\n",
      "LOCATION '/user/cloudera/retailentera/departments'\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514184848_bb5cf35f-86e6-4eca-8c57-1697c47b8bce); Time taken: 0.04 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514184848_bb5cf35f-86e6-4eca-8c57-1697c47b8bce): create external table departmentssqoopavro (\n",
      "  department_id  int,   \n",
      "  department_name  STRING) \n",
      "STORED AS AVRO\n",
      "LOCATION '/user/cloudera/retailentera/departments'\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514184848_bb5cf35f-86e6-4eca-8c57-1697c47b8bce); Time taken: 0.147 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.26 seconds)\n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop> \n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoop\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoop\" -f ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "create external table departmentssqoopavroB (\n",
    "  department_id  int,   \n",
    "  department_name  STRING) \n",
    "STORED AS AVRO\n",
    "LOCATION '/user/cloudera/retailentera/departmentsB';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:48:51,643 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 2ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoop\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "oopavroBhive2://localhost:10000/pruebasqoop> create external table departmentssq  (\n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop>   department_id  int,   \n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop>   department_name  STRING) \n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop> STORED AS AVRO\n",
      "ra/departmentsB';ocalhost:10000/pruebasqoop> LOCATION '/user/cloudera/retailente \n",
      "INFO  : Compiling command(queryId=hive_20190514184848_17462f31-7223-41fc-9cbc-8f91af1c314a): create external table departmentssqoopavroB (\n",
      "  department_id  int,   \n",
      "  department_name  STRING) \n",
      "STORED AS AVRO\n",
      "LOCATION '/user/cloudera/retailentera/departmentsB'\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514184848_17462f31-7223-41fc-9cbc-8f91af1c314a); Time taken: 0.02 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514184848_17462f31-7223-41fc-9cbc-8f91af1c314a): create external table departmentssqoopavroB (\n",
      "  department_id  int,   \n",
      "  department_name  STRING) \n",
      "STORED AS AVRO\n",
      "LOCATION '/user/cloudera/retailentera/departmentsB'\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20190514184848_17462f31-7223-41fc-9cbc-8f91af1c314a); Time taken: 0.06 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.133 seconds)\n",
      "0: jdbc:hive2://localhost:10000/pruebasqoop> \n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoop\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoop\" -f ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-14 18:49:47,865 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n",
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/pruebasqoop\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.7.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.7.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "INFO  : Compiling command(queryId=hive_20190514184949_a006d629-e88f-410b-8204-5dfc4f9dd05c): INSERT OVERWRITE TABLE departmentssqoopavroB SELECT * FROM departmentssqoopavro WHERE department_id = 3\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:departmentssqoopavro.department_id, type:int, comment:null), FieldSchema(name:departmentssqoopavro.department_name, type:string, comment:null)], properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20190514184949_a006d629-e88f-410b-8204-5dfc4f9dd05c); Time taken: 0.279 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20190514184949_a006d629-e88f-410b-8204-5dfc4f9dd05c): INSERT OVERWRITE TABLE departmentssqoopavroB SELECT * FROM departmentssqoopavro WHERE department_id = 3\n",
      "INFO  : Query ID = hive_20190514184949_a006d629-e88f-410b-8204-5dfc4f9dd05c\n",
      "INFO  : Total jobs = 3\n",
      "INFO  : Launching Job 1 out of 3\n",
      "INFO  : Starting task [Stage-1:MAPRED] in serial mode\n",
      "INFO  : Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "INFO  : Starting Job = job_1557856596151_0010, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1557856596151_0010/\n",
      "INFO  : Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1557856596151_0010\n",
      "INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "INFO  : 2019-05-14 18:49:56,109 Stage-1 map = 0%,  reduce = 0%\n",
      "INFO  : 2019-05-14 18:50:01,484 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.75 sec\n",
      "INFO  : MapReduce Total cumulative CPU time: 1 seconds 750 msec\n",
      "INFO  : Ended Job = job_1557856596151_0010\n",
      "INFO  : Starting task [Stage-7:CONDITIONAL] in serial mode\n",
      "INFO  : Stage-4 is selected by condition resolver.\n",
      "INFO  : Stage-3 is filtered out by condition resolver.\n",
      "INFO  : Stage-5 is filtered out by condition resolver.\n",
      "INFO  : Starting task [Stage-4:MOVE] in serial mode\n",
      "INFO  : Moving data to: hdfs://quickstart.cloudera:8020/user/cloudera/retailentera/departmentsB/.hive-staging_hive_2019-05-14_18-49-49_280_8859040824551440941-5/-ext-10000 from hdfs://quickstart.cloudera:8020/user/cloudera/retailentera/departmentsB/.hive-staging_hive_2019-05-14_18-49-49_280_8859040824551440941-5/-ext-10002\n",
      "INFO  : Starting task [Stage-0:MOVE] in serial mode\n",
      "INFO  : Loading data to table pruebasqoop.departmentssqoopavrob from hdfs://quickstart.cloudera:8020/user/cloudera/retailentera/departmentsB/.hive-staging_hive_2019-05-14_18-49-49_280_8859040824551440941-5/-ext-10000\n",
      "INFO  : Starting task [Stage-2:STATS] in serial mode\n",
      "INFO  : Table pruebasqoop.departmentssqoopavrob stats: [numFiles=1, numRows=1, totalSize=280, rawDataSize=0]\n",
      "INFO  : MapReduce Jobs Launched: \n",
      "INFO  : Stage-Stage-1: Map: 1   Cumulative CPU: 1.75 sec   HDFS Read: 7424 HDFS Write: 352 SUCCESS\n",
      "INFO  : Total MapReduce CPU Time Spent: 1 seconds 750 msec\n",
      "INFO  : Completed executing command(queryId=hive_20190514184949_a006d629-e88f-410b-8204-5dfc4f9dd05c); Time taken: 13.381 seconds\n",
      "INFO  : OK\n",
      "No rows affected (13.681 seconds)\n",
      "Beeline version 1.1.0-cdh5.7.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://localhost:10000/pruebasqoop\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/pruebasqoop\" -e \"INSERT OVERWRITE TABLE departmentssqoopavroB SELECT * FROM departmentssqoopavro WHERE department_id = 3;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos en el mysql la orden para crear la nueva tabla\n",
    "Esto no se muestra aquí. Tras eso ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemplo\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo\n",
    "# TU CODIGO PARA EL PUNTO 5 VA AQUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "19/05/14 18:58:46 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0\n",
      "19/05/14 18:58:46 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/05/14 18:58:46 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "19/05/14 18:58:46 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/05/14 18:58:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `depsB` AS t LIMIT 1\n",
      "19/05/14 18:58:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `depsB` AS t LIMIT 1\n",
      "19/05/14 18:58:47 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-root/compile/ada9d4763358bc6acf734358eb0f543b/depsB.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/05/14 18:58:48 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ada9d4763358bc6acf734358eb0f543b/depsB.jar\n",
      "19/05/14 18:58:48 INFO mapreduce.ExportJobBase: Beginning export of depsB\n",
      "19/05/14 18:58:48 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/05/14 18:58:48 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "19/05/14 18:58:48 INFO Configuration.deprecation: mapred.map.max.attempts is deprecated. Instead, use mapreduce.map.maxattempts\n",
      "19/05/14 18:58:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `depsB` AS t LIMIT 1\n",
      "19/05/14 18:58:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\n",
      "19/05/14 18:58:49 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative\n",
      "19/05/14 18:58:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/05/14 18:58:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/14 18:58:51 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "19/05/14 18:58:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/05/14 18:58:51 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative\n",
      "19/05/14 18:58:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557856596151_0011\n",
      "19/05/14 18:58:51 INFO impl.YarnClientImpl: Submitted application application_1557856596151_0011\n",
      "19/05/14 18:58:51 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1557856596151_0011/\n",
      "19/05/14 18:58:51 INFO mapreduce.Job: Running job: job_1557856596151_0011\n",
      "19/05/14 18:58:57 INFO mapreduce.Job: Job job_1557856596151_0011 running in uber mode : false\n",
      "19/05/14 18:58:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/14 18:59:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/14 18:59:03 INFO mapreduce.Job: Job job_1557856596151_0011 completed successfully\n",
      "19/05/14 18:59:03 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=138326\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=709\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=3\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2691\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=2691\n",
      "\t\tTotal vcore-seconds taken by all map tasks=2691\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=2755584\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tInput split bytes=145\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=21\n",
      "\t\tCPU time spent (ms)=770\n",
      "\t\tPhysical memory (bytes) snapshot=196325376\n",
      "\t\tVirtual memory (bytes) snapshot=1376583680\n",
      "\t\tTotal committed heap usage (bytes)=223870976\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=564\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "19/05/14 18:59:03 INFO mapreduce.ExportJobBase: Transferred 709 bytes in 13.6132 seconds (52.0819 bytes/sec)\n",
      "19/05/14 18:59:03 INFO mapreduce.ExportJobBase: Exported 1 records.\n"
     ]
    }
   ],
   "source": [
    "! sh ./ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que la exportación se ha realizado con éxito, ejecuta la siguiente orden, donde depsB es la tabla que hemos creado en mysql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\r\n",
      "| department_id | department_name |\r\n",
      "+---------------+-----------------+\r\n",
      "|             3 | Footwear        |\r\n",
      "+---------------+-----------------+\r\n"
     ]
    }
   ],
   "source": [
    "! mysql -u retail_dba -pcloudera -e \"use retail_db; select * from depsB\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
